/*
 * This Groovy source file was generated by the Gradle 'init' task.
 */
package imply

import com.opencsv.CSVParser
import com.opencsv.CSVWriter
import com.xlson.groovycsv.*
import org.apache.avro.file.*
import org.apache.avro.generic.*

import groovy.json.*

//java -cp Imply.jar challenges.DataSourceCompaction
//gradle run --args='../'

/*
The goal is to combine the files, eliminating any duplicates and write to a single .CSV file sorted alphabetically by the city name.
You can use any technology that you prefer.
The desired solution should be as generic, repeatable, and as automated as possible.

Once the dataset is completed, answer the following questions (and provide an explanation of how you determined your answer with any applicable code):
// TODO - what determines a duplicate? city/country only or city/country/population? How do we handle duplicates by removing them? What happens to the population?

What is the count of all rows?
What is the city with the largest population?
What is the total population of all cities in Brazil (CountryCode == BRA)?

What changes could be made to improve your program's performance.
- Async calls and process different file types simultaneously.
- Eliminating duplicates with hash collisions over utilizing several loops.
- Keep track of population while processing files to obtain highest population key.
- Keeping sort order while building list.

How would you scale your solution to a much larger dataset (too large for a single machine to store)?
- First pass would be to
- Second approach would be to simply leverage Apache Spark. The code below can easily be modified to a Java Spark job
for disributed computing of large datasets.

Your deliverable should be the following:
Your code to generate the dataset
A runbook with a guide on using your program
The dataset generated
Answers to the previous questions
*/


class DataSourceCompaction {


    final static String DEFAULT_BASE_DIR = "../conf"


    static void main(String[] args) {

        HashMap mergedDataSource = [:]
        if (args.size() == 0) {
            println "No directory argument given; Running Demo Mode."
            mergedDataSource = compactDataSources(DEFAULT_BASE_DIR)
        } else if (args.size() == 1) {
            println "Directory argument ${args[0]} passed in for processing."
            if (new File(args[0]).exists()) {
                mergedDataSource = compactDataSources(args[0])
            } else {
                println "ERROR: Argument ${new File(args[0]).absolutePath} not a valid directory."
                System.exit(0)
            }
        } else {
            println "ERROR: Too many arguments (${args.size()}) passed in, expected 1 or 0; Shutting down."
            System.exit(0)
        }

        if (mergedDataSource.size() > 0) {

            //What is the city with the largest population?
            def lisOfPopulations = mergedDataSource.collect { it.value.get("population") as Long }
            def largestPopulation = lisOfPopulations.max()
            def largestCity = mergedDataSource.find { key, value -> value.getAt("population") as Long == largestPopulation }
            println "The city with the largest population is: ${largestCity.value.getAt("name")}, ${largestCity.value.getAt("country")} with a population of ${largestCity.value.getAt("population")}"

            //What is the total population of all cities in Brazil (CountryCode == BRA)?
            def braTotalPopulation = 0;
            mergedDataSource.each {
                if (it.value.get("country").toString().equalsIgnoreCase("BRA")) {
                    //println "BRA: ${it.value.get("name").toString()} - ${it.value.get("population").toString()}"
                    braTotalPopulation += Long.parseLong(it.value.get("population").toString())
                } else {
                    return
                }
            }
            println "The total population for Brazil amongst the dataset is: ${braTotalPopulation}"
        }

    }

    //*********************************************************************************************************************//

    /**
     * Finds datasources to process where datasources are: .json, .csv, .avro files.
     *
     * @param folder
     * @return
     */
    def static findDataSourceFiles(File folder) {

        File[] avro = folder.listFiles(
                { dir, file -> file ==~ /.*\.[a][v][r][o]/ } as FilenameFilter
        )?.toList()
        println "Found ${avro.size()} .avro files in ${folder.path}."

        File[] json = folder.listFiles(
                { dir, file -> file ==~ /.*\.[j][s][o][n]/ } as FilenameFilter
        )?.toList()
        println "Found ${json.size()} .json files in ${folder.path}."

        File[] csv = folder.listFiles(
                { dir, file -> file ==~ /.*\.[c][s][v]/ } as FilenameFilter
        )?.toList()
        println "Found ${csv.size()} .csv files in ${folder.path}."

        return avro + csv + json
    }

    /**
     * Helper method to output given datasource data to a comma delimited, non-quoted .csv file
     *
     * @param fileData
     * @return
     */
    def static writeDataToCSV(Map fileData) {

        File file = new File("../CityList.${System.currentTimeMillis()}.csv");
        FileWriter outputfile = new FileWriter(file);
        CSVWriter writer = new CSVWriter(outputfile, CSVWriter.DEFAULT_SEPARATOR, CSVWriter.NO_QUOTE_CHARACTER,
                CSVWriter.DEFAULT_ESCAPE_CHARACTER, CSVWriter.RFC4180_LINE_END);

        String[] header = [ "Name", "CountryCode", "Population" ]
        writer.writeNext(header);

        fileData.each { key, value ->
            def city = value.getAt("name").toString()
            def country = value.getAt("country").toString()
            def pop = value.getAt("population").toString()
            String[] row = [city, country, pop]
            writer.writeNext(row)
        }
        writer.close();
    }

    /**
     * Helper method that parses data from a datasource file. Extracts Name, CountryCode and Population
     * while removing duplicate entries.
     *
     * @param fileToProcess
     * @param processedRecords
     * @return
     */
    def static processDataFromSource(File fileToProcess, HashMap processedRecords) {

        def totalRowsProcessed = 0
        def filename = fileToProcess.getName()
        CharSequence charSequence = new StringBuffer(".-_ ")
        def tokens = filename.tokenize(charSequence)

        //Switch on file type and parse data accordingly.
        def fileExt = tokens[tokens.size() - 1].toLowerCase()
        switch (fileExt) {
            case "avro":
                DataInputStream inputStream = fileToProcess.newDataInputStream()
                DataFileStream<GenericRecord> parsedFileData = new DataFileStream<>(inputStream, new GenericDatumReader<GenericRecord>())
                parsedFileData.each { GenericRecord line ->
                    def city = line.get("Name").toString().trim()
                    def country = line.get("CountryCode").toString().trim()
                    def pop = line.get("Population") as Long
                    processedRecords.put("${city},${country}", ["name" : city, "country" : country, "population" : pop])
                    totalRowsProcessed++
                }
                break
            case "json":
                def parser = new JsonSlurper().setType(JsonParserType.LAX)
                def parsedFileData = parser.parse(fileToProcess, "UTF-8")
                parsedFileData.each { line ->
                    def city = line.Name
                    def country = line.CountryCode
                    def pop = line.Population
                    processedRecords.put("${city},${country}", ["name" : city, "country" : country, "population" : pop])
                    totalRowsProcessed++
                }
            case "csv":
                def parsedFileData = new CsvParser().parse(fileToProcess.newReader(),
                        separator: CSVParser.DEFAULT_SEPARATOR, quoteChar: CSVParser.DEFAULT_QUOTE_CHARACTER, charset: "UTF-8")
                parsedFileData.each { line ->
                    def city = line.Name
                    def country = line.CountryCode
                    def pop = line.Population
                    processedRecords.put("${city},${country}", ["name": city, "country": country, "population": pop])
                    totalRowsProcessed++
                }
                break
            default:
                println "Bad file? Shouldn't happen..."
                break
        }

        println "Processed ${totalRowsProcessed} lines of data from file: $fileToProcess.name"
    }

    /**
     * Main method to run compaction on datasource files where compaction means: remove duplicates and
     * generate a single .csv datasource, sorted by the city field.
     *
     * @param BASE_DIR
     * @return
     */
    def static compactDataSources(String BASE_DIR) {

        def compactedData = [] as HashMap // Optimization to remove duplicates
        def highestPopulation = -1 //TODO Optimization to find highest pop while traversing through data vs. after.

        File folderToProcess = new File(BASE_DIR)

        if (folderToProcess.isDirectory()) {

            File[] filesToCompact = findDataSourceFiles(folderToProcess)
            println "Compacting ${filesToCompact.length} datasource files."

            //TODO Optimization for async
            filesToCompact.each {processDataFromSource(it, compactedData) }

            def sortedAndCompactedData = compactedData.sort { a, b ->
                a.value.get("name") <=> b.value.get("name")
            }

            writeDataToCSV(sortedAndCompactedData)

            println "Processed ${sortedAndCompactedData.size()} unique records from ${filesToCompact.size()} datasource files."
            return compactedData
        } else {
            println "Found a Directory $folderToProcess, processing folder."
            compactDataSources(folderToProcess)
        }

    }

}